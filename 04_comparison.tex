\chapter{各国のAI規制状況の比較}

前章では、AIに対する法規制の必要性を論じた。それを受け本章では、主要国・国際組織におけるAI規制の具体的な動向を比較分析する。

具体的には、包括的な法制化を進めるEU、分野別の規律を重視するアメリカ、そして韓国、中国、さらには国連における議論を取り上げ、そのアプローチの違いと共通点を明らかにする。

この比較検討を通じて、後の章で展開する日本のAI規制への提言に向けた、多角的な示唆を得ることを目的とする。

\section{諸外国との比較}

AIへの規制アプローチは、国や地域の事情を反映して多様な姿を見せている。本節ではその具体的な動向を概観するにあたり、まず世界で最も包括的かつ先行する法規制として知られるEUの「AI Act」を分析の起点とする。
これを基準とすることで、アメリカの分野別アプローチや、アジア各国の独自のアプローチとの違いがより明確になるからである。
%（※この章でどのようなものさしで比較するかについて述べる方法もあり）

\subsection{EU}
EUは2024年に世界で初めて包括的かつ法的拘束力を持つ「AI Act」を成立させ、AI規制に関する国際的な議論をリードする存在となっている。
その最大の特徴は、AIがもたらすリスクを4段階に分類し、リスクの程度に応じて異なる義務を課す「リスクベース・アプローチ」を基本設計に採用した点にある。
また適用範囲は域外適用の原則をとっており、EU域外の事業者にも影響を及ぼすものとなっている。

\subsubsection{禁止されるAI行為}
人間の尊厳や権利を侵害する、社会的に許容できないリスクをもたらすAIの利用を原則として禁止している。

主な禁止事項は以下の通りである。

\begin{itemize}
  \item 潜在意識に働きかけたり、操作したり、欺瞞的な手法を用いて行動を歪め、十分な情報に基づいた意思決定を阻害し、重大な危害を引き起こすこと
  \item 年齢、障害、または社会経済的状況に関連する脆弱性を悪用して行動を歪め、重大な危害を引き起こすこと
  \item 合法的に取得された生体認証データセットのラベル付けやフィルタリング、または法執行機関が生体認証データを分類する場合を除き、機密属性（人種、政治的意見、労働組合への加入、宗教的または哲学的信念、性生活、性的指向）を推測する生体認証分類システム
  \item 社会的スコアリング、すなわち、社会的行動や個人的な特徴に基づいて個人またはグループを評価または分類し、それらの人々に有害または不利な扱いを引き起こすこと
  \item プロファイリングや性格特性のみに基づいて個人が犯罪を犯すリスクを評価すること(ただし、犯罪行為に直接関連する客観的かつ検証可能な事実に基づく人間による評価を補強するために使用される場合は除く)
  \item インターネットや CCTV 映像から顔画像を無差別にスクレイピングして顔認識データベースを作成すること
  \item 医療上または安全上の理由を除き、職場や教育機関において感情を推測すること
  \item 法執行機関が公的にアクセス可能なスペースで「リアルタイム」のリモート生体認証 (RBI) を実施することはできない（テロ防止や重大犯罪の容疑者特定など、限定された例外有り）
\end{itemize}

\subsubsection{高リスクAIシステム}
社会や人々の権利に重大な影響を及ぼす可能性があるAIは「高リスク」と定義され、義務が課される。AIシステムが「高リスク」とみなされるのは、主に以下の2つのケースである。

\begin{itemize}
    \item EU法が定める製品の安全部品である場合:既存のEU法の対象となる製品、またはその安全部品としてAIが使われ、かつ第三者による適合性評価が求められる場合
    \item 特定の用途(ユースケース)に該当する場合:リストアップされている特定の分野・用途でAIが使用される場合(以下に記述)
\end{itemize}

以下のような分野・用途で使われるAIシステムがリストアップされている。

\begin{itemize}
    \item 生体認証:遠隔生体認証システム（個人が本人であると確認する認証は除く）、人々の属性や特性を推測する生体分類システム、感情認識システム
    \item 重要インフラ:道路交通や、水、ガス、暖房、電気の供給管理における安全コンポーネント
    \item 教育・職業訓練: 教育機関へのアクセスや入学の決定、学習成果の評価、テスト中の不正行為の監視
    \item 雇用・労働者管理: 求人広告、応募書類の分析・フィルタリング、候補者の評価といった採用活動。昇進や契約終了の決定、タスクの割り当て、パフォーマンスの監視・評価
    \item 公共サービス・民間サービスへのアクセス: 公的機関による給付金の受給資格の評価。信用力の評価（金融詐欺の検出は除く）。警察、消防、医療などの緊急通報の評価と優先順位付け。生命保険や健康保険のリスク評価と価格設定
    \item 法執行: 個人が犯罪被害者になるリスクの評価。ポリグラフ（嘘発見器）。犯罪捜査における証拠の信頼性評価。個人の犯罪リスク評価。犯罪捜査中のプロファイリング
    \item 移民・亡命・国境管理: ポリグラフ。不法移民や健康上のリスク評価。亡命、ビザ、居住許可の申請審査。個人の識別(渡航文書の検証は除く)
    \item 司法及び民主的プロセス: 事実の解釈や法律の適用にAIを使用すること。選挙結果や投票行動に影響を与えること(キャンペーンの最適化ツールなど、人間と直接対話しないものは除く)
\end{itemize}

ただし、上記に該当する場合でも、AIが「狭い手続き的なタスクを実行する」「人間が行った活動の結果を改善する」など、リスクが低いとみなされる特定の条件下では、ハイリスクから除外される例外がある。

重要な例外として上記に該当するAIシステムが、個人のプロファイリング(仕事のパフォーマンス、経済状況、健康、好み、行動などを評価するための個人データの自動処理)を行う場合は、常にハイリスクとみなされる。

\subsubsection{高リスクAIシステムの提供者に課される要求事項}

高リスクAIシステムの提供者は、以下の義務を果たす必要がある。

\begin{itemize}
    \item リスク管理システムの確立: AIシステムのライフサイクル全体を通じてリスクを管理する体制を構築すること
    \item データガバナンスの実施: 学習、検証、テストに用いるデータセットが、目的に対して適切かつ十分な代表性を持ち、可能な限りエラーがなく、完全であることを保証すること
    \item 技術文書の作成: 法令遵守を証明し、当局が評価するために必要な技術文書を準備すること
    \item 記録保持機能の設計: リスクを特定するために重要なイベント（ログ）を自動的に記録できるようシステムを設計すること
    \item 下流の展開者向け取扱説明書の提供: システムを導入する事業者（展開者）が法令を遵守できるよう、適切な使用方法を記載した説明書を提供すること
    \item 人間による監視（ヒューマン・オーバーサイト）の実装: 展開者が人間による適切な監視を行えるようシステムを設計すること
    \item 適切なレベルの正確性、堅牢性、サイバーセキュリティの確保
    \item 品質管理システムの確立: 法令遵守を確実にするための品質管理体制を構築すること
\end{itemize}

\subsubsection{新たな規制対象:汎用AI(GPAI)モデル}
AI Actは、特定の用途に限定されないGPAIモデル、特に社会に広範な影響を与える「システミックリスク」を伴うモデルに対して、新たな義務を課している。

\subsubsubsection{GPAIの定義}
GPAIの定義は以下の通りです。

\begin{itemize}
    \item 汎用AIモデル(GPAIモデル): 大量のデータを用いて自己教師あり学習などで訓練され、幅広いタスクを高い能力で実行できる、著しく汎用的なAIモデル。特定の製品として市場に出る前の、研究・開発・プロトタイピング段階のモデルは含まれない
    \item 汎用AIシステム(GPAIシステム): GPAIモデルを基盤とし、直接利用されたり、他のAIシステムに統合されたりするなど、多様な目的で利用できるAIシステム
\end{itemize}

\subsubsubsection{全てのGPAIモデル提供者に課される義務}
GPAIモデルを提供する全ての事業者は、以下の義務を負います。

\begin{itemize}
    \item 技術文書の作成: 訓練・テストのプロセスや評価結果を含む技術文書を作成すること
    \item 下流の提供者への情報提供: GPAIモデルを自社のAIシステムに統合しようとする事業者(下流の提供者)に対し、そのモデルの能力や限界を理解させ、法令遵守を可能にするための情報や文書を提供すること
    \item 著作権指令を尊重する方針の確立: EUの著作権法を遵守するための方針を策定すること
    \item 訓練コンテンツの要約の公開: モデルの訓練に使用したコンテンツについて、十分に詳細な要約を公開すること
\end{itemize}

パラメータ(重み、モデルのアーキテクチャ、使用法など)が公開されている自由かつオープンなライセンスのGPAIモデルは、上記の3つ目と4つ目の義務のみを遵守すればよいとされている。
ただし、そのモデルが後述の「システミックリスク」を持つ場合は、この例外は適用されない。

\subsubsubsection{システミックリスクを伴うGPAIモデル}
モデルの訓練に用いられた累積計算量が1025FLOPs(浮動小数点演算)を超える場合、「システミックリスク」を持つと見なされる。
その提供者は、この基準に達した場合、2週間以内に欧州委員会に通知する義務がある。

追加の義務は以下の通りである。

\begin{itemize}
    \item モデル評価の実施: システミックリスクを特定・軽減するため、敵対的テストを含むモデル評価を実施し、文書化すること
    \item システミックリスクの評価と軽減: 潜在的なシステミックリスクとその源泉を評価し、軽減措置を講じること
    \item 重大インシデントの追跡、文書化、報告: 重大なインシデントと是正措置の可能性について、遅滞なくAIオフィスおよび関連する国内管轄当局に報告すること
    \item サイバーセキュリティの確保: 適切なレベルのサイバーセキュリティ保護を保証すること
\end{itemize}


\subsubsection{限定的リスクと最小リスクAI}

上記以外にも、AI Actはリスクに応じた規律を定めている。

\begin{itemize}
    \item 限定的リスク:チャットボットやディープフェイクのように、人間がAIと対話していることを認識できない可能性がある場合、その旨を利用者に開示する透明性義務が課される
    \item 最小リスク:上記のいずれにも該当しない多くのAIアプリケーションは、最小リスクとみなされ、原則として規制の対象外となる
\end{itemize}


\subsubsection{執行と罰則}

AI Actの実効性を担保するため、EUレベルで監督を行う「AIオフィス」が設置される。
規則に違反した事業者には、最大で全世界の年間売上高の7\%または3500万ユーロのいずれか高い方が制裁金として科される可能性があり、極めて厳しい罰則規定となっている。

\subsection{アメリカ}

\subsection{中国}

\subsection{韓国}

\subsection{国連}

\section{各国のアプローチの違いと、メリットデメリットについて}
