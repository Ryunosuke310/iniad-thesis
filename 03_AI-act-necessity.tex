\chapter{AI規制法の必要性について}
本章では、AIに対する法的な規制がなぜ求められるのかを明らかにする。
そのために、まずAIの研究開発や利用に関する国際的な原則がどのように形成されてきたかを概観し、
次にAI規制に類似した生命倫理などの分野から得られる示唆を整理することで、AI規制の必要性を多角的に論じる。

\section{AI原則について} %05-1
AIガバナンスに関する国際的な議論は、技術の進展に伴い、理念の提示から具体的な政策指針へと深化してきた。
本節では、その流れを形作ったG7、G20、そして広島AIプロセスという3つの重要な国際合意を分析する。

しかし、本節の目的は、単にこれらの原則の歴史的変遷を追うことではない。むしろ、これらの原則が掲げる「透明性」や「安全性」、「アカウンタビリティ」を実現するための方法について焦点を当てる。
なぜなら、これらの理想的な価値観を社会の隅々にまで実効的に浸透させるためには、事業者の自主的な取り組みだけでは不十分だからである。
本節は、「原則が掲げる理想の内容そのものが、法的拘束力の必要性を示唆している」という視点から、AIに対する法規制(ハードロー)がなぜ求められるのか、その論拠を構築する。


\subsection{AIの研究開発の原則} 
AIガバナンスに関する国際的な議論の礎となったのが、2016年のG7香川・高松情報通信大臣会合で示された「AIの研究開発原則」である。
AIが社会経済に革命的な変化をもたらすとの認識のもと、G7各国が中心となり、AI開発者が留意すべき基本原則について議論が行われた。
%冒頭発言より引用しているため、あとでつなげるのを忘れずに
この原則は、以下の8つの項目で構成されている。

\begin{enumerate}
  \item 透明性の原則 : AIネットワークシステムの動作の説明可能性及び検証可能性を確保すること
  \item 利用者支援の原則 : AIネットワークシステムが利用者を支援するとともに、利用者に選択の機会を適切に提供するよう配慮すること
  \item 制御可能性の原則 : 人間によるAIネットワークシステムの制御可能性を確保すること
  \item セキュリティ確保の原則 : AIネットワークシステムの頑健性及び信頼性を確保すること
  \item 安全保護の原則 : AIネットワークシステムが利用者及び第三者の生命・身体の安全に危害を及ぼさないように配慮すること
  \item プライバシー保護の原則 : AIネットワークシステムが利用者及び第三者のプライバシーを侵害しないように配慮すること
  \item 倫理の原則 : ネットワーク化されるAIの研究開発において、人間の尊厳と個人の自立を尊重すること
  \item アカウンタビリティの原則 : ネットワーク化されるAIの研究開発者が利用者等関係ステークホルダーへのアカウンタビリティを果たすこと
\end{enumerate}

これらの原則は、AIがもたらすリスクに対して国際社会が初めて共通の方向性を示したという点で歴史的な意義を持つ。
特に、透明性やアカウンタビリティといった概念は、その後のEUのAI Actをはじめとする各国の法規制の議論において、
中核的な要素として引き継がれている。

なお、この「AIの研究開発原則」は、21年ぶりに開催されたG7香川・高松情報通信大臣会合において、
日本が主導的役割を果たし提案したものであり、その後の国際的なAI研究開発原則に関する議論の端緒となった、画期的な成果である。


\subsection{G20AI原則}
G7で示された研究開発原則の議論を踏まえ、具体的な検討は、経済協力開発機構(OECD)で行われ、2019年5月に42か国が「AIに関するOECD原則」を採択した。
この原則と同様の内容が2019年6月に開催されたG20大阪サミットで「G20 AI原則」として採択された。
OECD原則を基に、G20というより広範な国家間で承認された初のAI原則となった。

G7原則が開発者向けの理念であったのに対し、G20AI原則は各国政府が国内政策を推進する上での指針も含まれており、
AIガバナンスが具体的な制度設計の段階へと移行したことを示す重要な転換点といえる。
また、G20国家間でAIに対する考え方が必ずしも一致していない状況下において、G20として合意文書をまとめた成果は極めて大きい。

G20AI原則は、大きく5つの補完的な価値に基づく原則と、
5つの国内政策及び国際協力に関する勧告で構成されている。その全項目は以下のとおりである。

\subsection*{価値に関する原則} %目次に表示されません
\begin{description}
  \item[1. 包摂的な成長、持続可能な開発及び幸福]
  \item[2. 人間中心の価値観及び公平性]
  \item[3. 透明性及び説明可能性]
  \item[4. 頑健性、セキュリティ及び安全性]
  \item[5. アカウンタビリティ]
\end{description}

\subsection*{政策に関する勧告}
\begin{description}
  \item[1. AIの研究開発への投資]
  \item[2. AIのためのデジタル・エコシステムの育成]
  \item[3. AIを推進するための政策環境の整備]
  \item[4. 人材育成及び労働市場変革への準備]
  \item[5. 国際協力]
\end{description}

G20AI原則に示された項目の中でも、特に「1.2 人間中心の価値観及び公平性」「1.3 透明性及び説明可能性」「1.4 頑健性、セキュリティ及び安全性」「1..5 アカウンタビリティ」は、
AIを社会に実装する上での根源的な課題に応えようとするものであり、その後の各国の法規制の議論において中核的な論点であり続ける。
これら4つの原則は独立しているのではなく、相互に連携することで「信頼できるAI」という目標を達成するための論理的な枠組みを形成している。

\subsection{広島AIプロセス}

2019年のG20原則で確立された国際的な共通認識は、2022年末からの生成AIの爆発的な普及により、新たな局面を迎えることとなった。
G20原則が主に専門家による利用を想定した広範なAIを対象としていたのに対し、生成AIは一般市民にまで利用が拡大したため、
偽情報の拡散や著作権侵害といった問題が浮上した。

この新たな課題に対応するため、2023年にG7議長国であった日本が主導して立ち上げたのが「広島AIプロセス」である。
このプロセスの最大の意義は、生成AIに対応したガバナンスの国際的な枠組みを構築しようとした点にある。


広島AIプロセスで作成された「高度なAIシステムを開発する組織向けの広島プロセス国際行動規範」は以下のとおりである。

\begin{enumerate}
  \item AIライフサイクル全体にわたるリスクを特定、評価、軽減するために、高度なAIシステムの開発全体を通じて、その導入前及び市場投入前も含め、適切な措置を講じる。
  \item 市場投入を含む導入後、脆弱性、及び必要に応じて悪用されたインシデントやパターンを特定し緩和する。
  \item 高度なAIシステムの能力、限界、適切・不適切な使用領域を公表し、十分な透明性の確保を支援することで、アカウンタビリティの向上に貢献する。
  \item 産業界、政府、市民社会、学界を含む、高度なAIシステムを開発する組織間での責任ある情報共有とインシデントの報告に向けて取り組む。
  \item 個人情報保護方針及び緩和策を含む、リスクベースのアプローチに基づくAIガバナンス及びリスク管理方針を策定し、実施し、開示する。
  \item AIのライフサイクル全体にわたり、物理的セキュリティ、サイバーセキュリティ、内部脅威に対する安全対策を含む、強固なセキュリティ管理に投資し、実施する。
  \item 技術的に可能な場合は、電子透かしやそのほかの技術等、ユーザーがAIが生成したコンテンツを識別できるようにするための、信頼できるコンテンツ認証及び来歴のメカニズムを開発し、導入する。
  \item 社会的、安全、セキュリティ上のリスクを軽減するための研究を優先し、効果的な軽減策への投資を優先する。
  \item 世界の最大の課題、特に気候危機、世界保健、教育等(ただしこれらに限定されない)に対処するため、高度なAIシステムの開発を優先する。
  \item 国際的な技術規格の開発を推進し、適切な場合にはその採用を推進する。
  \item 適切なデータインプット対策を実施し、個人データおよび知的財産を保護する。
\end{enumerate}

広島AIプロセスは、G20原則で示された「人間中心」や「透明性」といった理念を引き継ぎつつも、議論の焦点をより具体的な「リスクの特定」と「対策の実践」へと移行させた。
その象徴的な成果が、先進的なAIシステムを開発する事業者向けの「国際行動規範」の策定である。
これは、AIガバナンスが抽象的な理念の共有から、具体的な行動を促すルールの策定へと、議論のフェーズを一段進めたことを示している。

\subsection{AI原則が示唆する法的拘束力の必要性}

G7、G20、広島AIプロセスを通じて国際社会が合意形成してきたAI原則は、AIと社会が共存するために不可欠な価値観を明確に示している。
しかし、本稿が問う「なぜ法規制が必要なのか」という点に立ち返ったとき、これらの原則は、その内容そのものが、法的拘束力の必要性を示唆していると指摘できる。

\subsubsection{「透明性」と「アカウンタビリティ」の担保}

G20原則が掲げる「透明性及び説明可能性」や「アカウンタビリティ」は、AIに対する社会的信頼の根幹である。
これは、AIシステムの判断によって、不利益を被った利用者が、その理由を知り、異議申し立てを行う権利を保障すべきだという考えである。

しかし、これが法的拘束力のない規制に留まるならば、事業者が営利上の理由（例：アルゴリズムの秘匿等）を優先し、説明を拒否した場合、利用者は対抗する手段を持たない。
「知る権利」や「意義を申し立てる権利」が真に実効性を持つためには、事業者に説明を義務付け、その不履行に罰則を科すといった、法的裏付けが不可欠となる。
つまり、原則が掲げる理想を実現するためには、法規制が必要なのである。

\subsubsection{「安全性」と「人間中心」の徹底}

同様に、「安全性」や「人間中心の価値観」も、自主的な取組だけでは徹底が難しい。
例として、自動運転や医療診断AIのように、人の生命・身体に直接的な影響を及ぼす高リスクな領域を考える。市場競争が激化する中で、全ての事業者がコストをかけて徹底的な安全性検証を行うとは限らない。
「守られるべき安全」という社会的な合意は、事業者の善意に期待するのではなく、法的な義務として課されることで初めて実効性を持つ。

\subsubsection{理想を実現するための「法」}

以上のように、国際社会が合意したAI原則は、「信頼できるAI社会」の理想的な姿を描き出している。
しかし、その理想を現実の社会で担保し、一部の不誠実な事業者によってその価値が損なわれないようにするためには、理念や原則を具体的な権利・義務へと落とし込む法制度が不可欠となる。

したがって、AI原則の存在は、「原則が掲げる崇高な理想を、社会の隅々まで行き渡らせるためにこそ、法的拘束力が必要なのだ」という、AI規制の必要性を物語る論拠となっているのである。


\section{生命倫理とAI倫理} %05-2

\subsection{フロリディの生命倫理}

\subsection{生命倫理とAI倫理の接点}

%ここいったん保留にします%

%\section{ソフトローとハードローの選択} %05-3

%\subsection{ソフトロー}

%\subsection{ハードロー}
