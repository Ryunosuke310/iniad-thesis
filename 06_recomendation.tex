\chapter{日本のAI法案に対する提言とAI兵器に対する日本の目指すべき立場}

\section{「最終報告(AI法案)」の概要}

2025年9月1日に施行された「人工知能関連技術の研究開発及び活用の推進に関する法律(以下、AI推進法)」は、
中間とりまとめの議論を経つつも、最終的には「イノベーションの促進」に極端に舵を切った内容となった。
本法の目的は「国民生活の向上」と「国民経済の健全な発展」にあり、AIに対する包括的な規制(禁止や罰則)ではなく、
研究開発や活用を国が支援するための基本法としての性格が色濃い。

\subsubsection*{目的}

\begin{itemize}
    \item 国民生活の向上
    \item 国民経済の発展
\end{itemize}

\subsubsection*{基本的な施策}

\begin{itemize}
    \item 研究開発の推進等
    \item 施設及び設備等の整備及び共用の促進
    \item 適正性の確保等
    \item 人材の確保等
    \item 教育の振興等
    \item 情報収集、権利利益を侵害する事案の分析、対策検討、調査研究等
    \item 事業者への指導、助言、情報提供
    \item 国際協力
\end{itemize}

\section{日本のAI法案に対する懸念と評価}

本法は「世界で最もAIを開発しやすい国」を目指す姿勢を明確にしており、短期的なイノベーションの活性化には寄与する可能性がある。
しかし、中間とりまとめで議論された重要な論点が抜け落ちており、以下の点で重大な懸念が残る。

\subsubsection*{1. リスクベースアプローチの欠落}
EUや韓国、そして米国でさえ事実上の標準として採用している「リスクベース・アプローチ」が、本法には明記されていない。
AIを一律に「推進すべき技術」として扱う現状は、医療や重要インフラ等に関わる高リスクAIが引き起こす事故や権利侵害への備えとして、極めて脆弱である。
本来、リスクベース・アプローチの意義は、重大な影響を及ぼすAIのみを厳格に管理し、それ以外を自由な開発領域として開放することにある。
つまり、このアプローチの導入こそが、規制への過度な萎縮を防ぎ、低リスク領域におけるイノベーションを強力に促進する土台となるのである。

\subsubsection*{2. 罰則と強制力の欠如}
本法案の最大の欠陥は、違反者に対する明確な罰則規定が存在せず、国による権限が「指導・助言」という強制力のない行政指導に留まっている点である。
日本政府は、技術革新のスピードに対応するため、柔軟な「ソフトロー（ガイドライン）」を中心とした統治を目指している。
しかし、ソフトローが機能するためには、その背後に「従わなければ法的制裁（ハードロー）が待っている」という確実な担保が必要不可欠である。
  
強制力を持たない現在の枠組みでは、「正直者が馬鹿を見る」不公正な競争環境の常態化という問題を引き起こすことが懸念される。
コンプライアンス意識の高い日本企業が、コストをかけて安全対策や倫理指針（ソフトロー）を遵守する一方で、利益優先の悪質な事業者や、
日本の行政指導に従うインセンティブを持たない国外事業者は、ルールを無視して低コストで開発・提供を行うことが可能となる。
これは、規制を守る者が市場競争で不利になるという「逆淘汰」を招き、健全な市場の発展を阻害する。
  
実効性のあるハードローという土台なきソフトローは砂上の楼閣に過ぎない。公正な競争環境と国民の安全を確保するためには、悪質な違反に対する十分な抑止力を持つ罰則規定の導入が不可欠である。

\subsubsection*{3. 国外事業者への対応不足}
日本市場で活動する国外事業者のAI(ChatGPT等)への対応についても、韓国のような「国内代理人制度」や明確な規定が欠落している点は看過できない。
これは、国内産業の保護と国民の権利擁護の両面において、法の重大な抜け穴となるリスクが高い。もっとも、韓国のような「国内代理人制度」の導入は、海外企業にとっての参入障壁となる懸念があり、日本での導入はハードルが高いと言える。
しかし、物理的な拠点を求めないとしても、法の適用範囲を国外にも拡張する「域外適用」を明記し、国外事業者に対しても罰則を含む国内法を適用できる法的根拠を確立することは不可欠である。

\section{規制項目の提案}

本論文では、現在の「AI推進法」の欠陥を補い、国際的な調和と国内のイノベーションを両立させるための、以下の4つの柱からなる新たな規制枠組みを提言する。

\subsection{リスクベース・アプローチの導入}

現在の一律的な推進ではなく、AIが人権や社会に与えるリスクの大きさに応じて、規制の強度を分類するアプローチを導入する。ただし、イノベーションを阻害しないよう、規制対象は必要最小限に留める。
また、分類は複雑化を避け、企業の予見可能性を高めるために「禁止」「高リスク」「低リスク」のシンプルな3段階とする。

\begin{description}
    \item [禁止されるAI] \mbox{} \\
    憲法が保障する基本的人権を著しく侵害するAI(例：公権力による無差別な顔認証監視、個人の社会的スコアリング)については、開発・利用を原則禁止とする。

    \item [高リスクAI] \mbox{} \\
    人命、身体、重要インフラに直結するAI(例：医療機器、自動運転、金融審査、採用選考等)を「特定AI」として指定し、事前の品質管理体制の構築と、事後の定期的な報告を義務付ける。

    \item [その他のAI] \mbox{} \\
    上記以外（例：ゲーム、スパムフィルター、業務効率化ツール等）は、原則として自由な開発を認める。事前の許可や届出は不要とするが、「市場監視」の対象都市、重大な事故や権利侵害が発生した際のインシデント報告および是正命令に従う義務のみを課す。

\end{description}



本提案におけるリスク分類は、EUのAI法(AI Act)を参考にしつつも、過度な細分化による複雑性を回避するため、「禁止」「高リスク」「低リスク」の3つのカテゴリーに集約した。
これは、包括的で厳格な規制を行うEUと、イノベーションを優先し柔軟な体制をとる韓国との中間に位置する、現実的な判断である。また、「禁止されるAI」の範囲設定には慎重な議論が求められるが、少なくとも憲法が保障する基本的人権を明白に侵害する利用形態については、
イノベーションの例外とせず、確実に法的規制の対象とする必要があるだろう。



\subsection{法の域外適用}

デジタル空間に国境はなく、日本国内で利用されるAIサービスの多くは国外事業者に依存している現状を踏まえ、物理的な拠点要件(代理人制度)に代わる、実効性のある管轄権の確保を提言する。

\subsubsection*{法の域外適用の明文化}

日本国内のユーザーに対してAIサービスを提供する事業者に対しては、その事業者が国内外のどこに所在しているかを問わず、本法を適用することを条文に明文化する。
適用の基準としては、サーバーの所在地にかかわらず、「日本国内にある者に対してAIサービスを提供している場合」または「日本国内の利用者のデータを監視・処理している場合」には、日本のAI規制法(リスク分類や透明性義務)が適用されるものとする。

\subsubsection*{提案の正当性：国際標準とデジタル主権}

この措置は、EUのAI法(AI Act)やGDPR(一般データ保護規則)でも採用されているように、デジタルの世界における国際的な標準慣行である。現在、日本の生成AI市場はOpenAIやGoogle等の米国企業に大きく依存しており、国民のデータや重要インフラの制御が実質的に国外の技術に委ねられている。
この状況下で国内法を適用しないことは、国家のデジタル主権を放棄することに等しい。したがって、域外適用は単なる規制強化ではなく、他国技術に依存しながらも、その安全性と倫理基準については日本の主権（法の支配）の下に置くための必須の措置である。
また、これにより、国内事業者と国外事業者が対等な条件下で競争する公平な市場環境を担保することができる。

\subsection{透明性の確保と利用者保護}

AI技術、特に生成AIの高度化により、真偽不明の情報や、人間と区別のつかない対話システムが社会に浸透している。これらが引き起こす「認知的混乱」や「権利侵害」を防ぐため、
現在は事業者の自主的な取り組み(ソフトロー)に委ねられている透明性確保措置を、法的拘束力のある義務(ハードロー)へと昇華させる必要がある。

\subsubsection*{生成AIコンテンツの表示義務：推奨から義務へ}

現在、総務省・経済産業省の「AI事業者ガイドライン(第1.0版)」においては、生成AIによる出力物に電子透かしやラベル等を付与し、AI生成物であることを明示することが「取り組むべき事項」として推奨されている。
しかし、偽情報やディープフェイクによる社会的な混乱（認知的安全保障の危機）を防ぐためには、このような努力義務では不十分である。なぜなら、コスト削減を優先する事業者や、悪意を持って偽情報を拡散させようとする主体は、法的強制力がない限り、こうした推奨を無視するインセンティブを持つからである。

\subsubsection*{ボットの通知義務：なりすましの防止}

人間と自然な対話を行うチャットボット等の普及に伴い、利用者が相手を人間であると誤認し、感情的に操作されたり、詐欺的行為に誘導されたりするリスクが高まっている。

このリスクを回避するためには、人間と対話するAIシステムを提供する際、利用者が「AIと対話している事実」を明確に認識できるよう、通知または表示を行うことを法的義務とする必要がある。
テキストベースの対話システムにおいては、UI(ユーザーインターフェース)上の表示によってこれを実装することは技術的に極めて容易であり、即座に義務化が可能である。この透明性の確保は、利用者がAIからの情報を批判的に吟味し、自律的な判断を下すための最低限の前提条件である。

\subsection{実効性の担保とイノベーションの調和}

規制の実効性を保ちつつ、開発を萎縮させないためには、ルールを破った者への「厳正な対処」と、挑戦する者への「安全な環境」をセットで提供する制度設計が必要である。

\subsubsection*{実効性のある罰則規定の導入と公表措置}

現行の「AI推進法」における国のアプローチは「指導・助言」に留まっており、悪質な事業者に対する強制力を持たない。これでは、法令遵守コストを回避するフリーライダーを助長し、真面目な事業者が不利益を被る恐れがある。
したがって、所管官庁による「是正命令」の権限を明記し、この命令に従わない場合や、虚偽の報告を行った場合に対する「罰則（懲役または罰金）」を規定すべきである。
また、物理的な強制執行が困難な国外事業者や、ブランド毀損を恐れる大企業に対する抑止力として、違反事業者の名称や違反内容を公開する「公表制度（公表措置）」を積極的に活用する。
これにより、過度な法改正を伴わずに、社会的な制裁による実効性を担保する。

\subsubsection*{EUに倣う「AI規制サンドボックス制度」の導入}

一方で、罰則の導入が開発の萎縮を招かないよう、EUのAI法で採用されている「規制サンドボックス」制度を導入する。本制度は、所管官庁の監督下において、期間限定・参加者限定で規制の一部を緩和し、革新的なAIシステムの実証実験を行える環境を提供するものである。
日本においても、特に「高リスクAI」に該当するか判断が難しい最先端技術について、いきなり市場投入して違法となるリスクを負わせるのではなく、まずサンドボックス内で安全性を検証するプロセスを設ける。
これにより、企業は「法的確実性」を得て安心して開発に投資でき、行政側は実験データに基づいた適切な規制の見直しが可能となる。

\section{平和国家としてのレッドライン:LAWSへの対応}

本章ではこれまで、市民生活やビジネスにおけるAI規制(民生利用)について論じてきた。
しかし、日本が真に安全なAI社会を構築するためには、避けて通れない「残された課題」がある。それは、EUのAI法をはじめとする主要国の規制において、「国家安全保障(軍事目的)」が適用除外とされている点である。
AI技術は本質的に、民間で開発された技術が容易に兵器転用可能な「デュアルユース性」を有している。したがって、民生用の規制だけでは不十分であり、軍事利用に対しても、日本の明確な立場(レッドライン)を定義する必要がある。

\subsection{LAWS(自律型致死兵器システム)に対する国際的合意の必要性}

特に議論の焦点となっているのが、人間の介入なしに標的を選択・攻撃する「LAWS(Lethal Autonomous Weapons Systems)」である。

現在、CCW(特定通常兵器使用禁止制限条約)の枠組みにおいて議論が進められているが、軍事大国間の対立により合意形成は難航している。
しかし、LAWSには看過できない2つのリスクが存在する。第一に「責任の所在の不明確さ」である。AIが誤作動やハルシネーションによって民間人を誤爆した場合、その責任を負うのは開発者か、指揮官か、あるいはAI自体か、という法的な空白が生じる。
第二に、「人間の尊厳の侵害」である。人間の生死にかかわる究極の判断を計算処理のアルゴリズムのみに委ねることは、人間の生命を単なるデータとして扱うことに他ならない。
この点において、「人間の制御」を欠いた完全自律型兵器は、決して譲ってはならない倫理的境界線（レッドライン）を超えるものである。

\subsection{日本がとるべき「AIの軍事利用」についての立場}

ここで日本が果たすべき役割は、単に欧米の軍事トレンドに追随することではない。
日本は、憲法第9条を持つ平和国家として、「完全自立型の殺傷兵器(LAWS)」は保有しない・開発しないという明確なコミットメントを世界に宣言し、国際法上で禁止対象とする「グローバルガバナンス」の構築に向けて、外交的なリーダーシップを発揮することが求められる。

国内の「AI推進法」やガイドラインにおいても、産業更新を謳う一方で、「AIの兵器利用における倫理的歯止め」を明記し、技術が暴走しないための国家としての意思を体現していくことが、国際社会に対する日本の責務である。



\section{日本が目指すべきAIガバナンスの方向性}

本章で提示した「4つの柱」からなる規制案は、推進一辺倒となっている現在の日本のAI政策に対し、国際的な整合性と倫理的な防波堤を組み込むための一つの試案である。もちろん、規制の強度や範囲については、多様な意見が存在することを筆者は認識している。
「過度な規制はイノベーションを阻害する」という産業界からの反論や、「さらに厳格な禁止が必要だ」という人権団体からの主張など、立場によって最適解は異なるだろう。

しかし、本論文で指摘した「リスクベース・アプローチの欠如」「国外事業者への強制力のなさ」「透明性の不備」といった課題、そして「軍事利用における明確なレッドラインのなさ」は、放置すれば国民の安全とデジタル主権を脅かす構造的な欠陥である。
したがって、筆者が提案した枠組みは、どのような立場に立つとしても、日本が「責任あるAI社会」を築くために避けては通れない最低限の必要条件(ミニマム・スタンダード)であると確信している。

AI技術は日々進化しており、固定的な法律ですべてを解決することは不可能である。だからこそ、日本政府には、現在の「推進法」で議論を終わらせることなく、本提案のような「規制とイノベーションの調和」を目指す具体的かつ建設的な議論を、継続して深化させていくことが強く求められる。

次章では、本論文全体の総括を行い、AIという「第四の革命」に対し、日本がどのような国家像を目指すべきかについて、最終的な結論を述べる。
